{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------- IMPORT LIBRARIES -------------------------#\n",
    "import torchvision, torch\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import alexnet\n",
    "\n",
    "# FILE DOVE METTIAMO TUTTE LE FUNZIONI CHE CI SERVONO, PER NON INTASARE IL MAIN\n",
    "from utils import *\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train Dataset: 25000\n",
      "Valid Dataset: 25000\n"
     ]
    }
   ],
   "source": [
    "# Uso = [1,8] o range(inizio, fine+1)\n",
    "# OGNI CLASSE HA 500 IMMAGINI: 250 TRAIN + 250 VALIDATION\n",
    "# SE VOGLIO UNA SOLO CLASSE NON USARE RANGE() MA mettere solo il numero classes=5\n",
    "train_dataloader, val_dataloader = get_train_val_dataloader(classes=range(0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "# Ogni classe ha 100 immagini\n",
    "test_dataloader  = get_test_dataloader(classes=range(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicoa/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=1000, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=100, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "# Implemento ResNet18 per il 1Â° punto del progetto\n",
    "#--------------------------------------------------\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', pretrained=False)\n",
    "# AlexNet has 1000 output neurons, corresponding to the 1000 ImageNet's classes\n",
    "# I need to change this because we have NUM_CLASSES\n",
    "##net.classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer\n",
    "                                                 # The convolutional layer is nn.Conv2d\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, NUM_CLASSES) \n",
    "\n",
    "# We just changed the last layer of AlexNet with a new fully connected layer with 100 outputs\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
    "\n",
    "# Choose parameters to optimize\n",
    "# To access a different set of parameters, you have to access submodules of AlexNet\n",
    "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
    "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
    "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
    "parameters_to_optimize = model.parameters() # In this case we optimize over all the parameters of AlexNet\n",
    "\n",
    "# Define optimizer\n",
    "# An optimizer updates the weights based on loss\n",
    "# We use SGD with momentum\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define scheduler\n",
    "# A scheduler dynamically changes learning rate\n",
    "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(DEVICE)\n",
    "cudnn.benchmark # Calling this optimizes runtime\n",
    "model, train_acc_history, train_loss_history, val_acc_history, val_loss_history = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VEDERE COME MAI SE AUMENTO IL NUMERO DI EPOCH LA LOSS INIZIALE AUMENTA\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(range(NUM_EPOCHS), val_loss_history, marker=\"s\", color=\"blue\")\n",
    "plt.plot(range(NUM_EPOCHS), train_loss_history, marker=\"s\", color=\"green\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xticks(range(0, NUM_EPOCHS, 1))\n",
    "plt.legend([\"validation\", \"train\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VEDERE COME MAI SE AUMENTO IL NUMERO DI EPOCH LA LOSS INIZIALE AUMENTA\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(range(NUM_EPOCHS), val_acc_history, marker=\"s\", color=\"blue\")\n",
    "plt.plot(range(NUM_EPOCHS), train_acc_history, marker=\"s\", color=\"green\")\n",
    "plt.xticks(range(0, NUM_EPOCHS, 1))\n",
    "plt.yticks(np.arange(0, 1.2, 0.2))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend([\"validation\", \"train\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
    "model.train(False) # Set Network to evaluation mode\n",
    "\n",
    "running_corrects = 0\n",
    "for images, labels in test_dataloader:\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    # Forward Pass\n",
    "    outputs = net(images)\n",
    "\n",
    "    # Get predictions\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Update Corrects\n",
    "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = running_corrects / float(len(test_dataset))\n",
    "print('Test Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n",
      "Files already downloaded and verified\n",
      "Train Dataset: 2500\n",
      "Valid Dataset: 2500\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 100, 10):\n",
    "    train_dataloader, val_dataloader = get_train_val_dataloader(classes=range(i, i+10))\n",
    "    # MODIFICARE ULTIMO LIVELLO PER AGGIUNGERE 10 CLASSI AL FC-CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
